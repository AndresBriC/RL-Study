{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSlotMachineEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(PairedSlotMachineEnv, self).__init__()\n",
    "\n",
    "        # Define action space: pick option 1 or option 2 in the current pair\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation space: represent each machine as an integer (0 to 7)\n",
    "        # We return the observation as a NumPy array\n",
    "        self.observation_space = spaces.MultiDiscrete([8, 8])\n",
    "\n",
    "        # Define slot machine points and probabilities\n",
    "        self.points = {\n",
    "            'A': 10, 'B': 10, 'C': 1, 'D': 1,\n",
    "            'E': 10, 'F': 10, 'G': 1, 'H': 1\n",
    "        }\n",
    "        self.probabilities = {\n",
    "            'A': 0.75, 'B': 0.25, 'C': 0.75, 'D': 0.25,\n",
    "            'E': 0.75, 'F': 0.25, 'G': 0.75, 'H': 0.25\n",
    "        }\n",
    "\n",
    "        # Original pairs for Learning Phase and new pairs for Transfer Phase\n",
    "        self.learning_pairs = [('A', 'B'), ('C', 'D'), ('E', 'F'), ('G', 'H')]\n",
    "        self.transfer_pairs = [('A', 'C'), ('B', 'D'), ('E', 'H'), ('F', 'G')]\n",
    "        self.transfer_phase = False  # Start in learning phase by default\n",
    "        self.current_pair = None\n",
    "\n",
    "    def set_transfer_phase(self, transfer: bool):\n",
    "        \"\"\"Toggle between learning and transfer phases.\"\"\"\n",
    "        self.transfer_phase = transfer\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Start with a random pair from the current phase\n",
    "        self.current_pair = self._get_random_pair()\n",
    "        # Return the initial observation as a NumPy array\n",
    "        return np.array([self._get_machine_index(self.current_pair[0]), \n",
    "                         self._get_machine_index(self.current_pair[1])]), {}\n",
    "\n",
    "    def _get_machine_index(self, machine):\n",
    "        # Map machine labels to indices for the observation space\n",
    "        machine_indices = {'A': 0, 'B': 1, 'C': 2, 'D': 3,\n",
    "                           'E': 4, 'F': 5, 'G': 6, 'H': 7}\n",
    "        return machine_indices[machine]\n",
    "\n",
    "    def _get_random_pair(self):\n",
    "        # Select from learning pairs or transfer pairs based on the phase\n",
    "        pairs = self.transfer_pairs if self.transfer_phase else self.learning_pairs\n",
    "        pair = pairs[np.random.randint(len(pairs))]\n",
    "        # Randomly shuffle the order with 50% probability\n",
    "        if np.random.rand() < 0.5:\n",
    "            pair = pair[::-1]\n",
    "        return pair\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action (0 or 1) to the selected machine in the current pair\n",
    "        chosen_machine = self.current_pair[action]\n",
    "        \n",
    "        # Determine the reward based on the chosen machineâ€™s probability\n",
    "        reward = self.points[chosen_machine] if np.random.rand() < self.probabilities[chosen_machine] else 0\n",
    "        \n",
    "        # Select a new random pair for the next step\n",
    "        self.current_pair = self._get_random_pair()\n",
    "        \n",
    "        # Return the updated observation and reward\n",
    "        observation = np.array([self._get_machine_index(self.current_pair[0]), \n",
    "                                self._get_machine_index(self.current_pair[1])])\n",
    "        return observation, reward, False, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Current pair: {self.current_pair[0]} vs {self.current_pair[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 436  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 364         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008721656 |\n",
      "|    clip_fraction        | 0.0264      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.00144     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 940         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 2.4e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007447136 |\n",
      "|    clip_fraction        | 0.0182      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | -0.00024    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.64e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00729    |\n",
      "|    value_loss           | 3.64e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 334         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010057544 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.00018     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 3.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008507195 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 8.47e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    value_loss           | 3.92e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 326        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00880448 |\n",
      "|    clip_fraction        | 0.0519     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.545     |\n",
      "|    explained_variance   | 8.52e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.93e+03   |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.00858   |\n",
      "|    value_loss           | 4.7e+03    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 315          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061273873 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 4.02e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.03e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00579     |\n",
      "|    value_loss           | 4.91e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 308          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059186732 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 6.44e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.31e+03     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 4.81e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001978946 |\n",
      "|    clip_fraction        | 0.0139      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | -2.62e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.25e+03    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 5.21e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 295         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004338298 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 7.27e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.89e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 5.79e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x232fa0d7470>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = PairedSlotMachineEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pair: D vs C\n",
      "Action taken: 1, Reward received: 1\n",
      "Current pair: F vs E\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: B vs A\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: A vs B\n",
      "Action taken: 0, Reward received: 10\n",
      "Current pair: G vs H\n",
      "Action taken: 1, Reward received: 1\n",
      "Current pair: B vs A\n",
      "Action taken: 1, Reward received: 10\n",
      "Current pair: H vs G\n",
      "Action taken: 1, Reward received: 1\n",
      "Current pair: C vs D\n",
      "Action taken: 0, Reward received: 1\n",
      "Current pair: A vs B\n",
      "Action taken: 0, Reward received: 10\n",
      "Current pair: D vs C\n",
      "Action taken: 1, Reward received: 0\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    print(f\"Action taken: {action}, Reward received: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pair: D vs B\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: E vs H\n",
      "Action taken: 0, Reward received: 10\n",
      "Current pair: E vs H\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: B vs D\n",
      "Action taken: 1, Reward received: 1\n",
      "Current pair: F vs G\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: C vs A\n",
      "Action taken: 1, Reward received: 10\n",
      "Current pair: A vs C\n",
      "Action taken: 0, Reward received: 10\n",
      "Current pair: E vs H\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: B vs D\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: A vs C\n",
      "Action taken: 0, Reward received: 10\n",
      "Current pair: G vs F\n",
      "Action taken: 0, Reward received: 1\n",
      "Current pair: D vs B\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: B vs D\n",
      "Action taken: 1, Reward received: 1\n",
      "Current pair: C vs A\n",
      "Action taken: 1, Reward received: 10\n",
      "Current pair: E vs H\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: D vs B\n",
      "Action taken: 0, Reward received: 0\n",
      "Current pair: H vs E\n",
      "Action taken: 1, Reward received: 10\n",
      "Current pair: B vs D\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: C vs A\n",
      "Action taken: 1, Reward received: 0\n",
      "Current pair: F vs G\n",
      "Action taken: 1, Reward received: 1\n"
     ]
    }
   ],
   "source": [
    "# Switch to Transfer Phase\n",
    "env.set_transfer_phase(True)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# Test agent in Transfer Phase\n",
    "for _ in range(20):\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    print(f\"Action taken: {action}, Reward received: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estanciaRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
