{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ImagePairEnvironment, self).__init__()\n",
    "        \n",
    "        # Define the base directory for images\n",
    "        self.image_dir = \"./ResizedSymbols\"\n",
    "\n",
    "         # Define image pairs with paths and points, using placeholders for paths\n",
    "        self.learning_pairs = {\n",
    "            'A+B': {'A': {'points': 10, 'prob': 0.75}, \n",
    "                    'B': {'points': 10, 'prob': 0.25}},\n",
    "            'C+D': {'C': {'points': 1, 'prob': 0.75}, \n",
    "                    'D': {'points': 1, 'prob': 0.25}},\n",
    "            'E+F': {'E': {'points': 10, 'prob': 0.75}, \n",
    "                    'F': {'points': 10, 'prob': 0.25}},\n",
    "            'G+H': {'G': {'points': 1, 'prob': 0.75}, \n",
    "                    'H': {'points': 1, 'prob': 0.25}}\n",
    "        }\n",
    "\n",
    "        self.transfer_pairs = {\n",
    "            'A+C': {'A': {'points': 10, 'prob': 0.75}, \n",
    "                    'C': {'points': 1, 'prob': 0.75}},\n",
    "            'B+D': {'B': {'points': 10, 'prob': 0.25}, \n",
    "                    'D': {'points': 1, 'prob': 0.25}},\n",
    "            'E+H': {'E': {'points': 10, 'prob': 0.75}, \n",
    "                    'H': {'points': 1, 'prob': 0.25}},\n",
    "            'F+G': {'F': {'points': 10, 'prob': 0.25}, \n",
    "                    'G': {'points': 1, 'prob': 0.75}}\n",
    "        }\n",
    "\n",
    "        self.phase = 'learning'\n",
    "        self.episode_count = 0\n",
    "        self.current_pair = None\n",
    "        \n",
    "        # Define the observation and action space\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(2, 400), dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(2)  # 0: choose left image, 1: choose right image\n",
    "\n",
    "    def get_image_path(self, symbol, has_indicator=False):\n",
    "        \"\"\"Return the path to the image file based on the symbol and indicator presence.\"\"\"\n",
    "        filename = f\"Symbol{symbol}{'_R' if has_indicator else ''}.png\"\n",
    "        return os.path.join(self.image_dir, filename)\n",
    "\n",
    "    def preprocess_image(self, path):\n",
    "        \"\"\"Load and preprocess the image from the given path.\"\"\"\n",
    "        img = Image.open(path).convert('L')  # Convert to grayscale\n",
    "        img_data = np.array(img).flatten() / 255.0  # Normalize pixel values\n",
    "        return img_data\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.episode_count = 0\n",
    "        if self.phase == 'learning':\n",
    "            self.current_pair = random.choice(list(self.learning_pairs.keys())) \n",
    "        elif self.phase == 'transfer':\n",
    "            self.current_pair = random.choice(list(self.transfer_pairs.keys()))\n",
    "        return self.get_image_observation(self.current_pair)\n",
    "    \n",
    "    def get_image_observation(self, pair_key):\n",
    "        # Swap the order 50% of the time\n",
    "        if random.random() < 0.5:\n",
    "            left_image, right_image = pair_key.split('+')\n",
    "        else:\n",
    "            right_image, left_image = pair_key.split('+')\n",
    "        \n",
    "        # Load and preprocess images from their respective paths\n",
    "        pair_data = self.learning_pairs if self.phase == 'learning' else self.transfer_pairs\n",
    "        \n",
    "        withSymbolL = np.random.rand() < pair_data[pair_key][left_image]['prob']\n",
    "        left_img_data = self.preprocess_image(self.get_image_path(left_image, withSymbolL))\n",
    "        \n",
    "        withSymbolR = np.random.rand() < pair_data[pair_key][right_image]['prob']\n",
    "        right_img_data = self.preprocess_image(self.get_image_path(right_image, withSymbolR))\n",
    "        \n",
    "        return np.array([left_img_data, right_img_data]), {'left_image': left_image, 'leftHasSymbol': withSymbolL, 'right_image': right_image, 'rightHasSymbol': withSymbolR,}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Identify which images are on the left and right\n",
    "        observation, images = self.get_image_observation(self.current_pair)\n",
    "        left_image = images['left_image']\n",
    "        right_image = images['right_image']\n",
    "\n",
    "        # Determine the chosen image\n",
    "        chosen_image = left_image if action == 0 else right_image\n",
    "        if action == 0:\n",
    "            chosen_image = left_image\n",
    "            hasSymbol = images['leftHasSymbol']\n",
    "        else:\n",
    "            chosen_image = right_image\n",
    "            hasSymbol = images['rightHasSymbol']\n",
    "\n",
    "        pair_data = self.learning_pairs[self.current_pair] if self.phase == 'learning' else self.transfer_pairs[self.current_pair]\n",
    "        \n",
    "        # Apply the 95% predictor rule\n",
    "        points = pair_data[chosen_image]['points']\n",
    "\n",
    "        # Hand a reward 95% of the times with a symbol and 5% of the times without a symbol\n",
    "        reward = points if (hasSymbol and np.random.rand() < 0.95) else (points if (not hasSymbol and np.random.rand() < 0.05) else 0)\n",
    "     \n",
    "        # Prepare next state\n",
    "        done = False\n",
    "        self.episode_count += 1\n",
    "        if self.phase == 'learning':\n",
    "            self.current_pair = random.choice(list(self.learning_pairs.keys())) \n",
    "        elif self.phase == 'transfer':\n",
    "            self.current_pair = random.choice(list(self.transfer_pairs.keys()))\n",
    "        \n",
    "        #if self.episode_count >= 10000:\n",
    "            #done = True\n",
    "        truncated = False\n",
    "        info = {'hasSymbol': hasSymbol}\n",
    "        \n",
    "        #print(self.episode_count)\n",
    "        return observation, reward, done, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 282  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 231          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014977283 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | -0.0013      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 905          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000862    |\n",
      "|    value_loss           | 2.12e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011238331 |\n",
      "|    clip_fraction        | 0.00527     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 2.07e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 3.12e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 233          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040685255 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.674       |\n",
      "|    explained_variance   | -1.41e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    value_loss           | 2.58e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005853487 |\n",
      "|    clip_fraction        | 0.0245      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -2.26e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.36e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    value_loss           | 2.98e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e05aca0fe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment and PPO agent\n",
    "env = ImagePairEnvironment()\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma=1)\n",
    "\n",
    "# Train the agent\n",
    "learning_timesteps = 10000  # Adjust if needed\n",
    "model.learn(total_timesteps=learning_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 27801\n",
      "Episode 2: Total Reward = 27123\n"
     ]
    }
   ],
   "source": [
    "# Set the environment to the transfer phase\n",
    "env.phase = 'transfer'\n",
    "vec_env = model.get_env()\n",
    "# Test the model\n",
    "num_episodes = 2  # Number of episodes to test\n",
    "runs = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = vec_env.reset()\n",
    "    #obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if env.episode_count == runs:\n",
    "            done = True\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward in transfer phase: 27454\n",
      "Percentage of True values: 50.51%\n"
     ]
    }
   ],
   "source": [
    "env.phase = 'transfer'  # Switch to transfer phase for testing\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "done = False\n",
    "rewards = []\n",
    "hasSymbol = np.empty(0)\n",
    "runs = 10000\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    hasSymbol = np.append(hasSymbol, info['hasSymbol'])\n",
    "    if env.episode_count == runs:\n",
    "        done = True\n",
    "\n",
    "print(f\"Total reward in transfer phase: {sum(rewards)}\")\n",
    "# Calculate the percentage of True values\n",
    "true_count = np.sum(hasSymbol)\n",
    "total_count = len(hasSymbol)\n",
    "percentage_true = (true_count / total_count) * 100\n",
    "\n",
    "print(f\"Percentage of True values: {percentage_true}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estanciaRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
